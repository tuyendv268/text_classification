{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57HNmbaCqc7_",
        "outputId": "dc9932e4-947f-470a-8735-96083df4d02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in c:\\users\\tuyen.dv\\anaconda3\\lib\\site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\tuyen.dv\\anaconda3\\lib\\site-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\tuyen.dv\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.17.0)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\tuyen.dv\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\tuyen.dv\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CSFyu3_rqgFq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "import gensim\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tdxooD7_qkuh"
      },
      "outputs": [],
      "source": [
        "with open(\"C:\\\\Users\\\\tuyen.dv\\\\Documents\\\\NLP\\\\NLP_tuan_3\\\\Data\\\\datasetVNTC.json\", encoding = \"utf-8\") as json_file:\n",
        "  data = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q67TuBNevG3G",
        "outputId": "4130e345-fbea-46af-9e9e-bf60fc622da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dân_số nhật bản giảm lần đầu_tiên trong lịch_sử chính_phủ nhật bản vừa báo_động dân_số nước này đã tụt giảm lần đầu_tiên trong lịch_sử kể từ khi nhật bắt_đầu thống_kê dân_số vào năm và đây là một dấu_hiệu xấu của một trong những xã_hội già_cỗi nhanh nhất thế_giới khảo_sát hằng năm của bộ tế cho thấy trong năm nay tử suất đã vượt sinh suất mức ngàn số_liệu trên quả là điều bất_ngờ cho các nhà nhân_khẩu_học vốn cho rằng thời_điểm sớm nhất mà dân_số nhật bắt_đầu giảm là vào năm trong khi trên thực_tế sự sụt_giảm này đã diễn ra nhanh hơn nguyên_nhân của tình_trạng trên là do sinh suất của xứ_sở phù tang quá thấp trẻ bà mẹ ngoài_ra những nguyên_nhân dẫn đến việc ngại sinh con là phụ_nữ nhật hiện có khuynh_hướng lập gia_đình muộn trong khi chỗ lại chật_hẹp và chi_phí giáo_dục cao dân_số tụt_dốc có_thể đe_dọa nghiêm_trọng đến nền kinh_tế vì nó gây ra tình_trạng thiếu_hụt lao_động thiếu_hụt nguồn thuế và gánh nặng phúc_lợi xã_hội khi tỷ_lệ người lao_động đóng thuế_giảm mạnh so với số người về hưu\n",
            "5\n",
            "thế giới\n"
          ]
        }
      ],
      "source": [
        "print(data['data'][5])\n",
        "print(data['target'][5])\n",
        "print(data['target_names'][5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "go_UapEgqvua"
      },
      "outputs": [],
      "source": [
        "def get_corpus(documents):\n",
        "    corpus = []\n",
        "    for i in range(len(documents)):\n",
        "        doc = documents[i]\n",
        "        words = doc.split(' ')\n",
        "        tagged_document = gensim.models.doc2vec.TaggedDocument(words, [i])\n",
        "        corpus.append(tagged_document)\n",
        "    return corpus\n",
        "    \n",
        "train_corpus = get_corpus(data['data'][0:30000])\n",
        "test_corpus = get_corpus(data['data'][30000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "peb0jhqSzuH_"
      },
      "outputs": [],
      "source": [
        "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=300, min_count=5, epochs=40)\n",
        "d2v_model.build_vocab(train_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2KCUgAAlzvXc"
      },
      "outputs": [],
      "source": [
        "d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qYez5NDTzxsV"
      },
      "outputs": [],
      "source": [
        "X_data_vectors = []\n",
        "for x in train_corpus:\n",
        "    vector = d2v_model.infer_vector(x.words)\n",
        "    X_data_vectors.append(vector)\n",
        "X_test_vectors = []\n",
        "for x in test_corpus:\n",
        "    vector = d2v_model.infer_vector(x.words)\n",
        "    X_test_vectors.append(vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OYrucpIDVNJX"
      },
      "outputs": [],
      "source": [
        "y_train = data['target'][0:30000]\n",
        "y_test = data['target'][30000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CJKeEMweWBM8"
      },
      "outputs": [],
      "source": [
        "#d2v_model.save(\"doc2vec.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "32OezyFG0c4A"
      },
      "outputs": [],
      "source": [
        "class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size,output_size):\n",
        "    super(MultilayerPerceptron, self).__init__()\n",
        "\n",
        "    # Saving the initialization parameters\n",
        "    self.input_size = input_size \n",
        "    self.output_size = output_size\n",
        "\n",
        "    # Defining model\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(self.input_size, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, output_size),\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    output = self.model(x)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STjabJp80p2f",
        "outputId": "22fe6ed9-1a6f-43da-adc9-157243934470"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30000, 300)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.asarray(X_data_vectors).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k6095d960ifz"
      },
      "outputs": [],
      "source": [
        "# Instantiate the model\n",
        "ann_model = MultilayerPerceptron(np.asarray(X_data_vectors).shape[1], 10)\n",
        "\n",
        "# Define the optimizer\n",
        "adam = optim.Adam(ann_model.parameters(), lr=0.1)#,weight_decay=1e-5)\n",
        "# sgd = optim.SGD(model.parameters(), lr = 0.01)\n",
        "# Define loss using a predefined loss function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Calculate how our model is doing now\n",
        "# y_pred = model(x)\n",
        "# loss_function(y_pred, y).item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5LhMnP7061V",
        "outputId": "fad16eec-3ff5-485f-8adf-15cfce524dc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\tuyen.dv\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
            "  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: traing loss: 2.3155720233917236\n",
            "Epoch 1: traing loss: 24.67447853088379\n",
            "Epoch 2: traing loss: 5.230480194091797\n",
            "Epoch 3: traing loss: 2.391493558883667\n",
            "Epoch 4: traing loss: 2.214385747909546\n",
            "Epoch 5: traing loss: 2.0549323558807373\n",
            "Epoch 6: traing loss: 1.968221664428711\n",
            "Epoch 7: traing loss: 1.9911746978759766\n",
            "Epoch 8: traing loss: 2.0307421684265137\n",
            "Epoch 9: traing loss: 1.9876731634140015\n",
            "Epoch 10: traing loss: 1.9430855512619019\n",
            "Epoch 11: traing loss: 1.9036471843719482\n",
            "Epoch 12: traing loss: 1.8654310703277588\n",
            "Epoch 13: traing loss: 1.8299050331115723\n",
            "Epoch 14: traing loss: 1.7826948165893555\n",
            "Epoch 15: traing loss: 1.736605167388916\n",
            "Epoch 16: traing loss: 1.6883610486984253\n",
            "Epoch 17: traing loss: 1.6590442657470703\n",
            "Epoch 18: traing loss: 1.625562310218811\n",
            "Epoch 19: traing loss: 1.5982879400253296\n",
            "Epoch 20: traing loss: 1.5733996629714966\n",
            "Epoch 21: traing loss: 1.5472300052642822\n",
            "Epoch 22: traing loss: 1.5185272693634033\n",
            "Epoch 23: traing loss: 1.4887983798980713\n",
            "Epoch 24: traing loss: 1.4588029384613037\n",
            "Epoch 25: traing loss: 1.4202910661697388\n",
            "Epoch 26: traing loss: 1.3766971826553345\n",
            "Epoch 27: traing loss: 1.3408311605453491\n",
            "Epoch 28: traing loss: 1.2989422082901\n",
            "Epoch 29: traing loss: 1.2629190683364868\n",
            "Epoch 30: traing loss: 1.2270933389663696\n",
            "Epoch 31: traing loss: 1.1920350790023804\n",
            "Epoch 32: traing loss: 1.160959005355835\n",
            "Epoch 33: traing loss: 1.1292685270309448\n",
            "Epoch 34: traing loss: 1.1008471250534058\n",
            "Epoch 35: traing loss: 1.075836181640625\n",
            "Epoch 36: traing loss: 1.0530732870101929\n",
            "Epoch 37: traing loss: 1.0299603939056396\n",
            "Epoch 38: traing loss: 1.0109611749649048\n",
            "Epoch 39: traing loss: 0.9885777831077576\n",
            "Epoch 40: traing loss: 0.9720197916030884\n",
            "Epoch 41: traing loss: 0.9647461175918579\n",
            "Epoch 42: traing loss: 1.0100544691085815\n",
            "Epoch 43: traing loss: 0.9958237409591675\n",
            "Epoch 44: traing loss: 0.9344692230224609\n",
            "Epoch 45: traing loss: 0.9431166052818298\n",
            "Epoch 46: traing loss: 0.9183537364006042\n",
            "Epoch 47: traing loss: 0.9171434640884399\n",
            "Epoch 48: traing loss: 0.8854234218597412\n",
            "Epoch 49: traing loss: 0.8895886540412903\n",
            "Epoch 50: traing loss: 0.8623220324516296\n",
            "Epoch 51: traing loss: 0.8637582659721375\n",
            "Epoch 52: traing loss: 0.8435519933700562\n",
            "Epoch 53: traing loss: 0.842149555683136\n",
            "Epoch 54: traing loss: 0.8284225463867188\n",
            "Epoch 55: traing loss: 0.8216999173164368\n",
            "Epoch 56: traing loss: 0.8147324323654175\n",
            "Epoch 57: traing loss: 0.8021087050437927\n",
            "Epoch 58: traing loss: 0.8001547455787659\n",
            "Epoch 59: traing loss: 0.7838278412818909\n",
            "Epoch 60: traing loss: 0.7905639410018921\n",
            "Epoch 61: traing loss: 0.7694623470306396\n",
            "Epoch 62: traing loss: 0.7733187675476074\n",
            "Epoch 63: traing loss: 0.75855952501297\n",
            "Epoch 64: traing loss: 0.7568058967590332\n",
            "Epoch 65: traing loss: 0.7532799243927002\n",
            "Epoch 66: traing loss: 0.740969717502594\n",
            "Epoch 67: traing loss: 0.739069402217865\n",
            "Epoch 68: traing loss: 0.7285473346710205\n",
            "Epoch 69: traing loss: 0.7411972880363464\n",
            "Epoch 70: traing loss: 0.7314648628234863\n",
            "Epoch 71: traing loss: 0.7216909527778625\n",
            "Epoch 72: traing loss: 0.7134876251220703\n",
            "Epoch 73: traing loss: 0.7203800678253174\n",
            "Epoch 74: traing loss: 0.7283867001533508\n",
            "Epoch 75: traing loss: 0.7419531345367432\n",
            "Epoch 76: traing loss: 0.7150828838348389\n",
            "Epoch 77: traing loss: 0.7017460465431213\n",
            "Epoch 78: traing loss: 0.7069967985153198\n",
            "Epoch 79: traing loss: 0.6923922300338745\n",
            "Epoch 80: traing loss: 0.6964025497436523\n",
            "Epoch 81: traing loss: 0.6780824065208435\n",
            "Epoch 82: traing loss: 0.6740818619728088\n",
            "Epoch 83: traing loss: 0.6621877551078796\n",
            "Epoch 84: traing loss: 0.6605201959609985\n",
            "Epoch 85: traing loss: 0.664726734161377\n",
            "Epoch 86: traing loss: 0.6517136096954346\n",
            "Epoch 87: traing loss: 0.6477093696594238\n",
            "Epoch 88: traing loss: 0.642724871635437\n",
            "Epoch 89: traing loss: 0.6327857375144958\n",
            "Epoch 90: traing loss: 0.6276124715805054\n",
            "Epoch 91: traing loss: 0.6173006892204285\n",
            "Epoch 92: traing loss: 0.6330212950706482\n",
            "Epoch 93: traing loss: 0.7062981128692627\n",
            "Epoch 94: traing loss: 0.6247661709785461\n",
            "Epoch 95: traing loss: 0.7305544018745422\n",
            "Epoch 96: traing loss: 0.6946607828140259\n",
            "Epoch 97: traing loss: 0.6871157288551331\n",
            "Epoch 98: traing loss: 0.6890414953231812\n",
            "Epoch 99: traing loss: 0.6552358269691467\n",
            "Epoch 100: traing loss: 0.6598984003067017\n",
            "Epoch 101: traing loss: 0.6672634482383728\n",
            "Epoch 102: traing loss: 0.6327944397926331\n",
            "Epoch 103: traing loss: 0.6311547160148621\n",
            "Epoch 104: traing loss: 0.6341292858123779\n",
            "Epoch 105: traing loss: 0.6232027411460876\n",
            "Epoch 106: traing loss: 0.6139795780181885\n",
            "Epoch 107: traing loss: 0.6082682013511658\n",
            "Epoch 108: traing loss: 0.6059293746948242\n",
            "Epoch 109: traing loss: 0.5987875461578369\n",
            "Epoch 110: traing loss: 0.5889252424240112\n",
            "Epoch 111: traing loss: 0.5890366435050964\n",
            "Epoch 112: traing loss: 0.5831828117370605\n",
            "Epoch 113: traing loss: 0.5752849578857422\n",
            "Epoch 114: traing loss: 0.5737519264221191\n",
            "Epoch 115: traing loss: 0.5673397183418274\n",
            "Epoch 116: traing loss: 0.5634371638298035\n",
            "Epoch 117: traing loss: 0.5617873668670654\n",
            "Epoch 118: traing loss: 0.5552659034729004\n",
            "Epoch 119: traing loss: 0.5507400035858154\n",
            "Epoch 120: traing loss: 0.5483699440956116\n",
            "Epoch 121: traing loss: 0.5449220538139343\n",
            "Epoch 122: traing loss: 0.5418617129325867\n",
            "Epoch 123: traing loss: 0.5392409563064575\n",
            "Epoch 124: traing loss: 0.5368987917900085\n",
            "Epoch 125: traing loss: 0.5346478223800659\n",
            "Epoch 126: traing loss: 0.532210111618042\n",
            "Epoch 127: traing loss: 0.5300535559654236\n",
            "Epoch 128: traing loss: 0.525986909866333\n",
            "Epoch 129: traing loss: 0.5246118903160095\n",
            "Epoch 130: traing loss: 0.5216554999351501\n",
            "Epoch 131: traing loss: 0.5196271538734436\n",
            "Epoch 132: traing loss: 0.5174458026885986\n",
            "Epoch 133: traing loss: 0.5162369608879089\n",
            "Epoch 134: traing loss: 0.5139994025230408\n",
            "Epoch 135: traing loss: 0.5136450529098511\n",
            "Epoch 136: traing loss: 0.5124745965003967\n",
            "Epoch 137: traing loss: 0.5181269645690918\n",
            "Epoch 138: traing loss: 0.5476018786430359\n",
            "Epoch 139: traing loss: 0.639683723449707\n",
            "Epoch 140: traing loss: 0.6248928904533386\n",
            "Epoch 141: traing loss: 0.5230972170829773\n",
            "Epoch 142: traing loss: 0.5715014934539795\n",
            "Epoch 143: traing loss: 0.5219564437866211\n",
            "Epoch 144: traing loss: 0.5578123331069946\n",
            "Epoch 145: traing loss: 0.5163998603820801\n",
            "Epoch 146: traing loss: 0.5476998686790466\n",
            "Epoch 147: traing loss: 0.5151631832122803\n",
            "Epoch 148: traing loss: 0.5284880995750427\n",
            "Epoch 149: traing loss: 0.5201275944709778\n",
            "Epoch 150: traing loss: 0.5150115489959717\n",
            "Epoch 151: traing loss: 0.520163893699646\n",
            "Epoch 152: traing loss: 0.5072203278541565\n",
            "Epoch 153: traing loss: 0.5135742425918579\n",
            "Epoch 154: traing loss: 0.5040729641914368\n",
            "Epoch 155: traing loss: 0.5073233246803284\n",
            "Epoch 156: traing loss: 0.5016383528709412\n",
            "Epoch 157: traing loss: 0.5017521977424622\n",
            "Epoch 158: traing loss: 0.5004664659500122\n",
            "Epoch 159: traing loss: 0.4944196343421936\n",
            "Epoch 160: traing loss: 0.4994108974933624\n",
            "Epoch 161: traing loss: 0.4913080334663391\n",
            "Epoch 162: traing loss: 0.49568799138069153\n",
            "Epoch 163: traing loss: 0.4893149137496948\n",
            "Epoch 164: traing loss: 0.4923924207687378\n",
            "Epoch 165: traing loss: 0.4871320128440857\n",
            "Epoch 166: traing loss: 0.4888994097709656\n",
            "Epoch 167: traing loss: 0.485588401556015\n",
            "Epoch 168: traing loss: 0.48563793301582336\n",
            "Epoch 169: traing loss: 0.48358678817749023\n",
            "Epoch 170: traing loss: 0.48334312438964844\n",
            "Epoch 171: traing loss: 0.4815612733364105\n",
            "Epoch 172: traing loss: 0.4811592996120453\n",
            "Epoch 173: traing loss: 0.479375958442688\n",
            "Epoch 174: traing loss: 0.47953182458877563\n",
            "Epoch 175: traing loss: 0.47751274704933167\n",
            "Epoch 176: traing loss: 0.4774843454360962\n",
            "Epoch 177: traing loss: 0.47547149658203125\n",
            "Epoch 178: traing loss: 0.47558510303497314\n",
            "Epoch 179: traing loss: 0.47374555468559265\n",
            "Epoch 180: traing loss: 0.4738677740097046\n",
            "Epoch 181: traing loss: 0.47220584750175476\n",
            "Epoch 182: traing loss: 0.4721778631210327\n",
            "Epoch 183: traing loss: 0.47086039185523987\n",
            "Epoch 184: traing loss: 0.4706290662288666\n",
            "Epoch 185: traing loss: 0.46970492601394653\n",
            "Epoch 186: traing loss: 0.469125360250473\n",
            "Epoch 187: traing loss: 0.468708336353302\n",
            "Epoch 188: traing loss: 0.467708557844162\n",
            "Epoch 189: traing loss: 0.467609703540802\n",
            "Epoch 190: traing loss: 0.4665253162384033\n",
            "Epoch 191: traing loss: 0.4662107825279236\n",
            "Epoch 192: traing loss: 0.46548768877983093\n",
            "Epoch 193: traing loss: 0.4648815095424652\n",
            "Epoch 194: traing loss: 0.4644518792629242\n",
            "Epoch 195: traing loss: 0.46379613876342773\n",
            "Epoch 196: traing loss: 0.46364179253578186\n",
            "Epoch 197: traing loss: 0.46362292766571045\n",
            "Epoch 198: traing loss: 0.4642273187637329\n",
            "Epoch 199: traing loss: 0.4669260084629059\n"
          ]
        }
      ],
      "source": [
        "n_epoch = 200\n",
        "for epoch in range(n_epoch):\n",
        "  # Set the gradients to 0\n",
        "  adam.zero_grad()\n",
        "  # Get the model predictions\n",
        "  y_pred = ann_model(torch.tensor(X_data_vectors, dtype = torch.float))\n",
        "  # Get the loss\n",
        "\n",
        "  loss = loss_function(y_pred, torch.tensor(y_train))\n",
        "  # Print stats\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "  # Compute the gradients\n",
        "  loss.backward()\n",
        "  # Take a step to optimize the weights\n",
        "  adam.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#torch.save(ann_model.state_dict(), \"ann_d2v1.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mC2LwZ8E1Sq-"
      },
      "outputs": [],
      "source": [
        "def evaluate(X_test, y_test):\n",
        "  pred = ann_model(torch.tensor(X_test, dtype = torch.float))\n",
        "  output = np.argmax(pred.tolist(), axis=1)\n",
        "  count = 0\n",
        "  n = X_test.shape[0]\n",
        "  for i in range(n):\n",
        "    if(output[i] == y_test[i]):\n",
        "      count += 1\n",
        "  return float(count)*100/n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IGp6yMNm1XUk",
        "outputId": "05bd3465-2069-4bf1-e9f3-34010d92b83a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84.59333333333333\n",
            "80.31391327480713\n"
          ]
        }
      ],
      "source": [
        "print(evaluate(np.asarray(X_data_vectors), y_train))\n",
        "print(evaluate(np.asarray(X_test_vectors), y_test))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ann_d2v.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
